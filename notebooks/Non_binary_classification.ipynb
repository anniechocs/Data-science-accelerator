{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This follows from the workbook \"New_binary_classification.ipynb\"\n",
    "## Based on the second half of Chapter 3 in O'Reilly book \"Hands on Machine Learning 2\"\n",
    "## This notebook is based on the notebook accompanying this chapter  \n",
    "The notebook is stored here on GCP, the relative path to this one is ../../handson-ml2/handson-ml2/03_classification.ipynb\n",
    "#### We read in the Fasttext embedding already created and apply binary classification following the chapter\n",
    "#### The notebook Non_binary_classification.ipynb follows on with non-binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# imbalanced dataset metrics\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import vectorized CPA data\n",
    "The vectorized columns are as follows:    \n",
    "- Descr_cleaned_vectorized : FastText vectorization of the \"Descr_cleaned\" column\n",
    "- Full_descr_cleaned_vectorized : FastText vectorization of the \"Full_descr_cleaned\" column\n",
    "- Descr_Low_dim : Reduced dimension (UMAP 10 dimensions) of FastText vectorization of the \"Descr_cleaned\" column\n",
    "- Full_descr_Low_dim : Reduced dimension (UMAP 10 dimensions) of FastText vectorization of the \"Full_descr_cleaned\" column\n",
    "\n",
    "The first two columns listed above hold the full vectorization (about 300 dim- check) and the \"Low_dim\" ones are reduced dimenstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Level</th>\n",
       "      <th>Descr_old</th>\n",
       "      <th>Descr</th>\n",
       "      <th>Includes</th>\n",
       "      <th>Category_0</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Full_descr</th>\n",
       "      <th>Descr_cleaned</th>\n",
       "      <th>Full_descr_cleaned</th>\n",
       "      <th>Descr_cleaned_vectorized</th>\n",
       "      <th>Full_descr_cleaned_vectorized</th>\n",
       "      <th>Descr_Low_dim</th>\n",
       "      <th>Full_descr_Low_dim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01.11.11</td>\n",
       "      <td>6</td>\n",
       "      <td>Durum wheat</td>\n",
       "      <td>Durum wheat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>Durum wheat</td>\n",
       "      <td>durum wheat</td>\n",
       "      <td>durum wheat</td>\n",
       "      <td>[-0.07021299  0.07487412 -0.08852207 -0.093424...</td>\n",
       "      <td>[-0.07021299  0.07487412 -0.08852207 -0.093424...</td>\n",
       "      <td>[10.613475799560547, 1.6112016439437866, 6.889...</td>\n",
       "      <td>[10.080077171325684, 3.1135828495025635, 5.750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01.11.12</td>\n",
       "      <td>6</td>\n",
       "      <td>Wheat, except durum wheat</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>Wheat, except durum wheat</td>\n",
       "      <td>wheat</td>\n",
       "      <td>wheat except durum wheat</td>\n",
       "      <td>[ 6.82471097e-02  5.72879892e-03 -2.74024643e-...</td>\n",
       "      <td>[-8.17044750e-02  4.43322510e-02 -1.21796057e-...</td>\n",
       "      <td>[10.610937118530273, 1.6146502494812012, 6.886...</td>\n",
       "      <td>[10.07783031463623, 3.1349775791168213, 5.7314...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01.11.20</td>\n",
       "      <td>6</td>\n",
       "      <td>Maize</td>\n",
       "      <td>Maize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>Maize</td>\n",
       "      <td>maize</td>\n",
       "      <td>maize</td>\n",
       "      <td>[-2.75462180e-01  2.88966715e-01 -1.60463899e-...</td>\n",
       "      <td>[-2.75462180e-01  2.88966715e-01 -1.60463899e-...</td>\n",
       "      <td>[10.598653793334961, 1.5319408178329468, 6.899...</td>\n",
       "      <td>[10.088605880737305, 3.1780178546905518, 5.732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01.11.31</td>\n",
       "      <td>6</td>\n",
       "      <td>Barley</td>\n",
       "      <td>Barley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>Barley</td>\n",
       "      <td>barley</td>\n",
       "      <td>barley</td>\n",
       "      <td>[-0.48283365 -0.02554321 -0.09297911  0.358175...</td>\n",
       "      <td>[-0.48283365 -0.02554321 -0.09297911  0.358175...</td>\n",
       "      <td>[10.606842994689941, 1.6108537912368774, 6.884...</td>\n",
       "      <td>[10.082571983337402, 3.0978615283966064, 5.779...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>01.11.32</td>\n",
       "      <td>6</td>\n",
       "      <td>Rye</td>\n",
       "      <td>Rye</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>Rye</td>\n",
       "      <td>rye</td>\n",
       "      <td>rye</td>\n",
       "      <td>[-0.23049644  0.07332443 -0.51851004  0.030723...</td>\n",
       "      <td>[-0.23049644  0.07332443 -0.51851004  0.030723...</td>\n",
       "      <td>[10.645343780517578, 1.672475814819336, 6.8885...</td>\n",
       "      <td>[10.067237854003906, 3.0663797855377197, 5.772...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Code  Level                  Descr_old        Descr Includes  \\\n",
       "5   01.11.11      6                Durum wheat  Durum wheat      NaN   \n",
       "6   01.11.12      6  Wheat, except durum wheat        Wheat      NaN   \n",
       "8   01.11.20      6                      Maize        Maize      NaN   \n",
       "10  01.11.31      6                     Barley       Barley      NaN   \n",
       "11  01.11.32      6                        Rye          Rye      NaN   \n",
       "\n",
       "    Category_0 Category_1  Category_2                 Full_descr  \\\n",
       "5            1          A           1                Durum wheat   \n",
       "6            1          A           1  Wheat, except durum wheat   \n",
       "8            1          A           1                      Maize   \n",
       "10           1          A           1                     Barley   \n",
       "11           1          A           1                        Rye   \n",
       "\n",
       "   Descr_cleaned        Full_descr_cleaned  \\\n",
       "5    durum wheat               durum wheat   \n",
       "6          wheat  wheat except durum wheat   \n",
       "8          maize                     maize   \n",
       "10        barley                    barley   \n",
       "11           rye                       rye   \n",
       "\n",
       "                             Descr_cleaned_vectorized  \\\n",
       "5   [-0.07021299  0.07487412 -0.08852207 -0.093424...   \n",
       "6   [ 6.82471097e-02  5.72879892e-03 -2.74024643e-...   \n",
       "8   [-2.75462180e-01  2.88966715e-01 -1.60463899e-...   \n",
       "10  [-0.48283365 -0.02554321 -0.09297911  0.358175...   \n",
       "11  [-0.23049644  0.07332443 -0.51851004  0.030723...   \n",
       "\n",
       "                        Full_descr_cleaned_vectorized  \\\n",
       "5   [-0.07021299  0.07487412 -0.08852207 -0.093424...   \n",
       "6   [-8.17044750e-02  4.43322510e-02 -1.21796057e-...   \n",
       "8   [-2.75462180e-01  2.88966715e-01 -1.60463899e-...   \n",
       "10  [-0.48283365 -0.02554321 -0.09297911  0.358175...   \n",
       "11  [-0.23049644  0.07332443 -0.51851004  0.030723...   \n",
       "\n",
       "                                        Descr_Low_dim  \\\n",
       "5   [10.613475799560547, 1.6112016439437866, 6.889...   \n",
       "6   [10.610937118530273, 1.6146502494812012, 6.886...   \n",
       "8   [10.598653793334961, 1.5319408178329468, 6.899...   \n",
       "10  [10.606842994689941, 1.6108537912368774, 6.884...   \n",
       "11  [10.645343780517578, 1.672475814819336, 6.8885...   \n",
       "\n",
       "                                   Full_descr_Low_dim  \n",
       "5   [10.080077171325684, 3.1135828495025635, 5.750...  \n",
       "6   [10.07783031463623, 3.1349775791168213, 5.7314...  \n",
       "8   [10.088605880737305, 3.1780178546905518, 5.732...  \n",
       "10  [10.082571983337402, 3.0978615283966064, 5.779...  \n",
       "11  [10.067237854003906, 3.0663797855377197, 5.772...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to ensure the columns of lists are imported as such\n",
    "import ast\n",
    "generic = lambda x: ast.literal_eval(x)\n",
    "conv = {\n",
    "        'Descr_Low_dim': generic,\n",
    "        'Full_descr_Low_dim': generic}\n",
    "CPA = pd.read_csv('../data/output/CPA_Vectorized.csv', converters=conv)\n",
    "\n",
    "CPA = CPA[CPA.Level==6]\n",
    "CPA = CPA.astype({'Category_0':int,'Category_2':int})\n",
    "CPA.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the CPA data into a training and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2574, 10) (2574,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10.24176407,  1.495134  ,  6.934062  ,  5.49697828,  5.47507858,\n",
       "         6.19090939,  9.82301331,  7.17451763,  6.48560524,  4.21679831],\n",
       "       [ 6.43087292,  2.48803687,  1.78880703,  3.02287507,  6.34812403,\n",
       "         3.15756965,  0.56744212,  3.84727097,  6.04655981,  1.59090734]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the CPA dataframe into train and test\n",
    "train_set, test_set = train_test_split(CPA, test_size=0.2, random_state=42)\n",
    "# The vector column will be the data we use for classification\n",
    "col = 'Descr_Low_dim'\n",
    "X_train = np.array(list(train_set[col]))\n",
    "X_test = np.array(list(test_set[col]))\n",
    "\n",
    "# we will use different levels of categorisation\n",
    "# we can choose the level of categorisation here, Category_0, _1 or _2, and also the vectorisation to use\n",
    "Cat = 'Category_0'\n",
    "y_train_cat0 = np.array(list(train_set[Cat]))\n",
    "y_test_cat0 = np.array(list(test_set[Cat]))\n",
    "Cat = 'Category_1'\n",
    "y_train_cat1 = np.array(list(train_set[Cat]))\n",
    "y_test_cat1 = np.array(list(test_set[Cat]))\n",
    "Cat = 'Category_2'\n",
    "y_train_cat2 = np.array(list(train_set[Cat]))\n",
    "y_test_cat2 = np.array(list(test_set[Cat]))\n",
    "\n",
    "print(X_train.shape, y_train_cat1.shape)\n",
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.392472267150879,\n",
       " 2.1317131519317627,\n",
       " 7.459656238555908,\n",
       " 6.61699104309082,\n",
       " 4.1032819747924805,\n",
       " 6.102914333343506,\n",
       " 9.116209030151367,\n",
       " 9.040961265563965,\n",
       " 6.673974990844727,\n",
       " 3.864978790283203]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To follow the book, let's set up a test item with categories we know\n",
    "test_item = CPA[(CPA.Category_2==10)&(CPA.Category_1=='C')&(CPA.Category_0==2)].Descr_Low_dim[438]\n",
    "test_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Data Metrics\n",
    "Our data is imbalanced in the sense that the numbers of items in each class varies a lot.   \n",
    "This section diagresses from the book.\n",
    "\n",
    "NOTE: I had quite a lot of difficulty getting VS Code to recognise imblearn. Finally it worked using !pip install imblearn in jupyter cell directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "Whereas binary classifiers distinguish between two classes, multiclass classifiers (also called multinomial classifiers) can distinguish between more than two classes.\n",
    "\n",
    "Some algorithms (such as Logistic Regression classifiers, Random Forest classifiers, and naive Bayes classifiers) are capable of handling multiple classes natively. Others (such as SGD Classifiers or Support Vector Machine classifiers) are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification with multiple binary classifiers.\n",
    "\n",
    "** One-versus-rest **   \n",
    "One way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs the highest score. This is called the one-versus-the-rest (OvR) strategy (also called one-versus-all).\n",
    "\n",
    "** One-versus-one **   \n",
    "Another strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers! When you want to classify an image, you have to run the image through all 45 classifiers and see which class wins the most duels. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.\n",
    "\n",
    "Some algorithms (such as Support Vector Machine classifiers) *scale poorly with the size of the training set*. For these algorithms OvO is preferred because it is faster to train many classifiers on small training sets than to train few classifiers on large training sets. ** For most binary classification algorithms, however, OvR is preferred. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine classifier\n",
    "Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvR or OvO, depending on the algorithm. Let’s try this with a Support Vector Machine classifier (see Chapter 5), using the sklearn.svm.SVC class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, Scikit-Learn actually used the OvO strategy: it trained N * (N-1) binary classifiers, got their decision scores for the image, and selected the class that won the most duels.\n",
    "\n",
    "If you call the decision_function() method, you will see that it returns N scores per instance (instead of just 1). That’s one score per class (it’s the number of won duels plus or minus a small tweak to break ties, based on the binary classifier scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Category_0 : target is 2nd \n",
      " [[ 8.29784893  9.30140582  2.73598047  7.27305884  1.72272008  0.71475454\n",
      "  -0.29611218  6.20043305  3.81638767  4.99894874]]\n",
      "Scores for Category_1 : target is 3rd \n",
      " [[19.31761568  9.94805749 20.31813263  6.71574784 12.18839512 10.92955875\n",
      "  18.31215185 16.28514249  4.71007455  8.73576367  3.70409808  1.68890782\n",
      "  15.28555923 17.29297943  2.69948015  5.74240527 13.22725539  7.7599955\n",
      "  14.28103271  0.69076363 -0.31505816]]\n",
      "Scores for Category_1 : target is 3rd \n",
      " [[86.32977312 73.32235829 85.32908422  7.67329075 49.26075995 21.67703709\n",
      "  24.67983317 33.68305329 87.32989489 77.32027593 75.3156973  80.32626072\n",
      "  27.68346952 84.32805105 56.30773453 72.32399326 36.69054155 73.32402056\n",
      "  80.32882196 49.22943733 54.28419396 62.31280486 52.28066299 66.32220154\n",
      "  61.31582348 62.31865833 83.32922604 54.26984546 67.32302221 28.67887789\n",
      "  80.32849547 76.32646244 49.19184403  3.67388919  4.67423176 70.32283417\n",
      "   5.67444593 43.98628358 54.26737476 37.69147784 34.68499953 78.32746455\n",
      "  82.32851182 63.31485819 11.67475617 15.67494405 67.32191163 34.69655469\n",
      "  29.68371334 27.68357175 42.71199689 38.69297095 10.67448086 14.6745286\n",
      "  35.69310846 24.67848986 15.67512279 31.68910616 19.67563367  7.67413595\n",
      "  22.67945348 16.67483328 59.3164035   8.67479748 40.71471746 48.29345101\n",
      "  16.67434155 59.31119739 13.67461573 52.29907031 25.67882637 63.31832961\n",
      "  62.31811169 32.69115457 51.29723701 70.32216824 16.67504756 18.67574243\n",
      "   3.6740446  43.9516548  12.67459962 35.70724997 42.7065383  47.87052375\n",
      "  70.3201169   0.67462421  1.67462408 -0.32779181]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svm_clf0 = SVC(gamma=\"auto\", random_state=42)\n",
    "svm_clf1 = SVC(gamma=\"auto\", random_state=42)\n",
    "svm_clf2 = SVC(gamma=\"auto\", random_state=42)\n",
    "svm_clf0.fit(X_train, y_train_cat0)\n",
    "print('Scores for Category_0 : target is 2nd \\n',svm_clf0.decision_function([test_item]))\n",
    "svm_clf1.fit(X_train, y_train_cat1)\n",
    "print('Scores for Category_1 : target is 3rd \\n',svm_clf1.decision_function([test_item]))\n",
    "svm_clf2.fit(X_train, y_train_cat2)\n",
    "print('Scores for Category_1 : target is 3rd \\n',svm_clf2.decision_function([test_item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Category_0:  [0.85314685 0.84615385 0.85547786]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for Category_1:  [0.82400932 0.82517483 0.81701632]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for Category_2:  [0.57342657 0.53030303 0.55011655]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for Category_0: ',cross_val_score(svm_clf0, X_train, y_train_cat0, cv=3, scoring=\"accuracy\"))\n",
    "print('\\nAccuracy for Category_1: ',cross_val_score(svm_clf1, X_train, y_train_cat1, cv=3, scoring=\"accuracy\"))\n",
    "print('\\nAccuracy for Category_2: ',cross_val_score(svm_clf2, X_train, y_train_cat2, cv=3, scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  78,    0,   78,    0,    0,    0,    0,    0,    0,    1,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    2,    0,    0],\n",
       "       [   0,    0,   29,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [  30,    1, 1330,    0,    0,    1,   15,   14,    0,   14,    0,\n",
       "           0,    2,    3,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    7,    3,    0,    0,    1,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,   11,    0,   47,    0,    0,    0,    0,    0,    0,\n",
       "           0,    1,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,   19,    0,    0,   43,    1,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    2,    1,    0,    0,    0],\n",
       "       [   0,    0,   19,    0,    0,    0,  162,    0,    0,    5,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    4,    0,    0,    0,    0,   77,    1,    4,    1,\n",
       "           0,    1,    8,    0,    0,    1,    0,    0,    1,    0],\n",
       "       [   0,    0,    0,    0,    0,    1,    0,    0,   11,    0,    0,\n",
       "           2,    0,    1,    0,    0,    1,    1,    0,    1,    0],\n",
       "       [   0,    0,    8,    0,    0,    0,    0,    0,    0,   96,    0,\n",
       "           0,    4,    0,    0,    0,    2,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    4,    0,    0,    0,   60,\n",
       "           0,    5,    2,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          15,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    5,    0,    0,    0,    1,    3,    0,    9,    6,\n",
       "           0,   81,    6,    0,    0,    1,    1,    0,    0,    0],\n",
       "       [   0,    0,    5,    0,    2,    0,    3,    0,    1,    7,    1,\n",
       "           0,    9,   38,    0,    0,    2,    0,    3,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
       "           0,    3,   18,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    2,    0,   22,    2,    3,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    2,    0,    0,    0,    0,\n",
       "           0,    3,    1,    0,    0,   29,    0,    0,    0,    0],\n",
       "       [   0,    0,    2,    0,    0,    1,    0,    0,    0,    7,    0,\n",
       "           0,    1,    1,    0,    0,    2,   13,    0,    0,    0],\n",
       "       [   0,    0,   10,    0,    0,    0,    7,    0,    1,    0,    0,\n",
       "           0,    2,    7,    0,    0,    6,    0,    7,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    4,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    1,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_tmp = cross_val_predict(svm_clf1, X_train, y_train_cat1, cv=3)\n",
    "conf_mx_tmp = confusion_matrix(y_train_cat1, y_train_pred_tmp)\n",
    "conf_mx_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          A       0.72      0.49      0.99      0.58      0.70      0.46       159\n",
      "          B       0.00      0.00      1.00      0.00      0.00      0.00        29\n",
      "          C       0.87      0.94      0.83      0.91      0.89      0.79      1410\n",
      "          D       1.00      0.27      1.00      0.43      0.52      0.25        11\n",
      "          E       0.96      0.80      1.00      0.87      0.89      0.78        59\n",
      "          F       0.93      0.65      1.00      0.77      0.81      0.63        66\n",
      "          G       0.83      0.87      0.99      0.85      0.93      0.85       186\n",
      "          H       0.82      0.79      0.99      0.80      0.88      0.76        98\n",
      "          I       0.79      0.61      1.00      0.69      0.78      0.59        18\n",
      "          J       0.67      0.87      0.98      0.76      0.93      0.85       110\n",
      "          K       0.87      0.85      1.00      0.86      0.92      0.83        71\n",
      "          L       0.88      1.00      1.00      0.94      1.00      1.00        15\n",
      "          M       0.72      0.72      0.99      0.72      0.84      0.69       113\n",
      "          N       0.44      0.54      0.98      0.48      0.72      0.50        71\n",
      "          O       0.00      0.00      1.00      0.00      0.00      0.00        22\n",
      "          P       1.00      0.76      1.00      0.86      0.87      0.74        29\n",
      "          Q       0.59      0.83      0.99      0.69      0.91      0.81        35\n",
      "          R       0.68      0.48      1.00      0.57      0.69      0.46        27\n",
      "          S       0.58      0.17      1.00      0.27      0.42      0.16        40\n",
      "          T       0.67      1.00      1.00      0.80      1.00      1.00         4\n",
      "          U       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.81      0.82      0.90      0.81      0.84      0.73      2574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report_imbalanced(y_train_cat1, y_train_pred_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat this for the Category_0 and Category_2 classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          1       0.72      0.49      0.99      0.58      0.70      0.46       159\n",
      "          2       0.91      0.95      0.86      0.93      0.90      0.82      1509\n",
      "          3       0.96      0.67      1.00      0.79      0.82      0.64        66\n",
      "          4       0.78      0.88      0.97      0.83      0.92      0.84       302\n",
      "          5       0.68      0.85      0.98      0.75      0.91      0.82       110\n",
      "          6       0.88      0.85      1.00      0.86      0.92      0.83        71\n",
      "          7       0.94      1.00      1.00      0.97      1.00      1.00        15\n",
      "          8       0.72      0.65      0.98      0.68      0.80      0.61       184\n",
      "          9       0.79      0.74      0.99      0.77      0.86      0.72        86\n",
      "         10       0.67      0.36      0.99      0.47      0.60      0.34        72\n",
      "\n",
      "avg / total       0.85      0.85      0.91      0.85      0.87      0.77      2574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          1       0.63      0.65      0.98      0.64      0.80      0.62       132\n",
      "          2       0.00      0.00      1.00      0.00      0.00      0.00         7\n",
      "          3       0.00      0.00      1.00      0.00      0.00      0.00        20\n",
      "          5       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "          6       0.00      0.00      1.00      0.00      0.00      0.00         3\n",
      "          7       0.00      0.00      1.00      0.00      0.00      0.00         7\n",
      "          8       0.00      0.00      1.00      0.00      0.00      0.00        15\n",
      "          9       0.00      0.00      1.00      0.00      0.00      0.00         3\n",
      "         10       0.55      0.73      0.96      0.63      0.84      0.69       156\n",
      "         11       0.67      0.46      1.00      0.55      0.68      0.44        13\n",
      "         12       0.00      0.00      1.00      0.00      0.00      0.00         4\n",
      "         13       0.48      0.84      0.98      0.61      0.90      0.81        68\n",
      "         14       0.80      0.76      1.00      0.78      0.87      0.74        37\n",
      "         15       0.00      0.00      1.00      0.00      0.00      0.00        24\n",
      "         16       0.46      0.15      1.00      0.23      0.39      0.14        39\n",
      "         17       0.58      0.62      0.99      0.60      0.78      0.59        47\n",
      "         18       0.00      0.00      1.00      0.00      0.00      0.00         9\n",
      "         19       0.50      0.10      1.00      0.16      0.31      0.09        21\n",
      "         20       0.67      0.81      0.98      0.73      0.89      0.78       140\n",
      "         21       0.00      0.00      1.00      0.00      0.00      0.00        17\n",
      "         22       0.10      0.03      1.00      0.04      0.16      0.02        38\n",
      "         23       0.33      0.69      0.96      0.44      0.81      0.64        70\n",
      "         24       0.70      0.95      0.98      0.80      0.97      0.93        95\n",
      "         25       0.26      0.08      0.99      0.13      0.29      0.08        72\n",
      "         26       0.57      0.83      0.97      0.67      0.90      0.80        99\n",
      "         27       0.78      0.50      1.00      0.61      0.71      0.47        84\n",
      "         28       0.45      0.71      0.93      0.55      0.81      0.65       187\n",
      "         29       0.22      0.17      0.99      0.19      0.41      0.15        30\n",
      "         30       0.35      0.14      1.00      0.20      0.37      0.12        44\n",
      "         31       0.00      0.00      1.00      0.00      0.00      0.00        19\n",
      "         32       0.50      0.05      1.00      0.10      0.23      0.05        55\n",
      "         33       0.88      0.55      1.00      0.68      0.74      0.52        42\n",
      "         35       1.00      0.27      1.00      0.43      0.52      0.25        11\n",
      "         36       0.00      0.00      1.00      0.00      0.00      0.00         3\n",
      "         37       0.00      0.00      1.00      0.00      0.00      0.00         2\n",
      "         38       0.82      0.82      1.00      0.82      0.90      0.80        49\n",
      "         39       0.00      0.00      1.00      0.00      0.00      0.00         5\n",
      "         41       0.75      0.83      1.00      0.79      0.91      0.82        18\n",
      "         42       1.00      0.04      1.00      0.08      0.21      0.04        23\n",
      "         43       0.68      0.68      1.00      0.68      0.82      0.66        25\n",
      "         45       0.52      0.94      0.99      0.67      0.96      0.92        33\n",
      "         46       0.57      0.88      0.97      0.69      0.93      0.85       100\n",
      "         47       0.00      0.00      1.00      0.00      0.00      0.00        53\n",
      "         49       0.45      0.81      0.99      0.57      0.89      0.78        31\n",
      "         50       0.50      0.85      0.99      0.63      0.92      0.83        20\n",
      "         51       0.00      0.00      1.00      0.00      0.00      0.00        11\n",
      "         52       1.00      0.07      1.00      0.13      0.27      0.06        28\n",
      "         53       1.00      0.25      1.00      0.40      0.50      0.23         8\n",
      "         55       1.00      0.67      1.00      0.80      0.82      0.64         9\n",
      "         56       0.50      0.33      1.00      0.40      0.58      0.31         9\n",
      "         58       0.52      0.85      0.99      0.65      0.92      0.83        41\n",
      "         59       0.37      0.60      0.99      0.45      0.77      0.57        25\n",
      "         60       0.00      0.00      1.00      0.00      0.00      0.00         7\n",
      "         61       0.71      1.00      1.00      0.83      1.00      1.00        17\n",
      "         62       0.00      0.00      1.00      0.00      0.00      0.00        10\n",
      "         63       0.00      0.00      1.00      0.00      0.00      0.00        10\n",
      "         64       0.94      0.65      1.00      0.77      0.81      0.63        23\n",
      "         65       0.57      0.83      0.99      0.68      0.91      0.81        30\n",
      "         66       0.24      0.39      0.99      0.30      0.62      0.36        18\n",
      "         68       0.88      1.00      1.00      0.94      1.00      1.00        15\n",
      "         69       0.00      0.00      1.00      0.00      0.00      0.00        11\n",
      "         70       0.00      0.00      1.00      0.00      0.00      0.00         9\n",
      "         71       0.44      0.74      0.99      0.55      0.86      0.71        23\n",
      "         72       0.84      1.00      1.00      0.92      1.00      1.00        38\n",
      "         73       0.00      0.00      1.00      0.00      0.00      0.00        12\n",
      "         74       0.19      0.16      0.99      0.17      0.40      0.14        19\n",
      "         75       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "         77       0.56      0.65      1.00      0.60      0.81      0.63        23\n",
      "         78       0.00      0.00      1.00      0.00      0.00      0.00        10\n",
      "         79       0.44      0.36      1.00      0.40      0.60      0.34        11\n",
      "         80       0.00      0.00      1.00      0.00      0.00      0.00         5\n",
      "         81       0.00      0.00      1.00      0.00      0.00      0.00        11\n",
      "         82       0.00      0.00      1.00      0.00      0.00      0.00        11\n",
      "         84       0.39      0.86      0.99      0.54      0.92      0.84        22\n",
      "         85       0.96      0.76      1.00      0.85      0.87      0.74        29\n",
      "         86       1.00      0.62      1.00      0.77      0.79      0.60        16\n",
      "         87       0.00      0.00      1.00      0.00      0.00      0.00         7\n",
      "         88       0.31      0.92      0.99      0.47      0.95      0.90        12\n",
      "         90       0.50      0.20      1.00      0.29      0.45      0.18         5\n",
      "         91       0.00      0.00      1.00      0.00      0.00      0.00         6\n",
      "         92       1.00      0.17      1.00      0.29      0.41      0.15         6\n",
      "         93       0.35      0.60      1.00      0.44      0.77      0.57        10\n",
      "         94       0.79      0.79      1.00      0.79      0.89      0.77        14\n",
      "         95       0.00      0.00      1.00      0.00      0.00      0.00        10\n",
      "         96       0.00      0.00      1.00      0.00      0.00      0.00        16\n",
      "         97       1.00      0.50      1.00      0.67      0.71      0.48         2\n",
      "         98       0.33      0.50      1.00      0.40      0.71      0.47         2\n",
      "         99       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.50      0.55      0.98      0.49      0.66      0.53      2574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_0 = cross_val_predict(svm_clf0, X_train, y_train_cat0, cv=3)\n",
    "print(classification_report_imbalanced(y_train_cat0, y_train_pred_0))\n",
    "y_train_pred_2 = cross_val_predict(svm_clf2, X_train, y_train_cat2, cv=3)\n",
    "print(classification_report_imbalanced(y_train_cat2, y_train_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Multi classifier\n",
    "Training an SGDClassifier is just as easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Category_0:  [0.68181818 0.79137529 0.72494172]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for Category_1:  [0.75291375 0.71794872 0.74825175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for Category_2:  [0.28088578 0.16433566 0.21095571]\n"
     ]
    }
   ],
   "source": [
    "sgd_clf0 = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf1 = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf2 = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "sgd_clf0.fit(X_train, y_train_cat0)\n",
    "sgd_clf1.fit(X_train, y_train_cat1)\n",
    "sgd_clf2.fit(X_train, y_train_cat2)\n",
    "\n",
    "print('Accuracy for Category_0: ',cross_val_score(sgd_clf0, X_train, y_train_cat0, cv=3, scoring=\"accuracy\"))\n",
    "print('\\nAccuracy for Category_1: ',cross_val_score(sgd_clf1, X_train, y_train_cat1, cv=3, scoring=\"accuracy\"))\n",
    "print('\\nAccuracy for Category_2: ',cross_val_score(sgd_clf2, X_train, y_train_cat2, cv=3, scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving results using a scaler\n",
    "These results are already a lot better than a random classifier.   \n",
    "QUESTION: since our data is already vectorized, is using this scalar meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Category_0:  [0.7972028  0.78088578 0.76223776]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for Category_1:  [0.75058275 0.75641026 0.75291375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for Category_2:  [0.35431235 0.41608392 0.38111888]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "\n",
    "print('Accuracy for Category_0: ',cross_val_score(sgd_clf0, X_train_scaled, y_train_cat0, cv=3, scoring=\"accuracy\"))\n",
    "print('\\nAccuracy for Category_1: ',cross_val_score(sgd_clf1, X_train_scaled, y_train_cat1, cv=3, scoring=\"accuracy\"))\n",
    "print('\\nAccuracy for Category_2: ',cross_val_score(sgd_clf2, X_train_scaled, y_train_cat2, cv=3, scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  67,    0,   88,    0,    0,    0,    0,    0,    0,    1,    0,\n",
       "           0,    0,    3,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,   27,    1,    0,    0,    0,    1,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [  60,    1, 1297,    1,    0,    2,    5,   23,    0,   14,    0,\n",
       "           0,    1,    3,    0,    0,    0,    1,    2,    0,    0],\n",
       "       [   0,    0,    6,    4,    0,    0,    1,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    9,    1,   47,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    2,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,   14,    0,    0,   40,    1,    4,    0,    0,    0,\n",
       "           0,    0,    7,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   1,    0,   57,    0,    0,    0,  123,    0,    0,    5,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   1,    0,    6,    0,    0,    0,    2,   78,    0,    5,    1,\n",
       "           0,    0,    4,    0,    0,    0,    0,    0,    1,    0],\n",
       "       [   0,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0,\n",
       "           0,    0,   13,    0,    0,    0,    0,    3,    1,    0],\n",
       "       [   0,    0,    8,    0,    0,    0,    0,    0,    0,   98,    0,\n",
       "           0,    0,    4,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    4,    0,    0,    0,    0,    1,    0,    1,   55,\n",
       "           0,    0,    9,    0,    0,    0,    0,    1,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    1,    0,    5,\n",
       "           5,    0,    4,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    5,    0,    0,    1,    1,    3,    0,   18,    5,\n",
       "           0,   57,   22,    0,    0,    0,    0,    1,    0,    0],\n",
       "       [   0,    0,    7,    0,    2,    0,   10,    5,    0,    9,    1,\n",
       "           0,    4,   28,    0,    0,    1,    0,    4,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
       "           0,    2,   15,    0,    0,    0,    0,    4,    0,    0],\n",
       "       [   1,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0,\n",
       "           0,    0,    5,    0,   21,    0,    0,    1,    0,    0],\n",
       "       [   0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0,\n",
       "           0,    1,   15,    0,    1,   14,    0,    2,    0,    0],\n",
       "       [   0,    0,    2,    0,    0,    7,    0,    0,    0,    7,    0,\n",
       "           0,    1,    9,    0,    0,    0,    1,    0,    0,    0],\n",
       "       [   0,    0,   16,    0,    0,    0,    1,    0,    0,    2,    0,\n",
       "           0,    3,   11,    4,    0,    1,    0,    2,    0,    0],\n",
       "       [   0,    0,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    1,    2,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    1,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_1 = cross_val_predict(sgd_clf1, X_train_scaled, y_train_cat1, cv=3)\n",
    "conf_mx = confusion_matrix(y_train_cat1, y_train_pred_1)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          A       0.52      0.42      0.97      0.46      0.64      0.39       159\n",
      "          B       0.00      0.00      1.00      0.00      0.00      0.00        29\n",
      "          C       0.84      0.92      0.78      0.88      0.85      0.73      1410\n",
      "          D       0.57      0.36      1.00      0.44      0.60      0.34        11\n",
      "          E       0.96      0.80      1.00      0.87      0.89      0.78        59\n",
      "          F       0.77      0.61      1.00      0.68      0.78      0.58        66\n",
      "          G       0.85      0.66      0.99      0.74      0.81      0.63       186\n",
      "          H       0.68      0.80      0.99      0.73      0.89      0.77        98\n",
      "          I       0.00      0.00      1.00      0.00      0.00      0.00        18\n",
      "          J       0.61      0.89      0.97      0.73      0.93      0.86       110\n",
      "          K       0.81      0.77      0.99      0.79      0.88      0.75        71\n",
      "          L       1.00      0.33      1.00      0.50      0.58      0.31        15\n",
      "          M       0.83      0.50      1.00      0.63      0.71      0.48       113\n",
      "          N       0.18      0.39      0.95      0.25      0.61      0.35        71\n",
      "          O       0.00      0.00      1.00      0.00      0.00      0.00        22\n",
      "          P       0.95      0.72      1.00      0.82      0.85      0.70        29\n",
      "          Q       0.88      0.40      1.00      0.55      0.63      0.38        35\n",
      "          R       0.50      0.04      1.00      0.07      0.19      0.03        27\n",
      "          S       0.10      0.05      0.99      0.07      0.22      0.04        40\n",
      "          T       0.50      0.50      1.00      0.50      0.71      0.47         4\n",
      "          U       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.75      0.75      0.88      0.74      0.78      0.64      2574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report_imbalanced(y_train_cat1, y_train_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          1       0.48      0.57      0.96      0.52      0.74      0.52       159\n",
      "          2       0.90      0.88      0.87      0.89      0.87      0.77      1509\n",
      "          3       0.74      0.64      0.99      0.68      0.80      0.61        66\n",
      "          4       0.67      0.79      0.95      0.73      0.87      0.74       302\n",
      "          5       0.61      0.86      0.98      0.71      0.92      0.83       110\n",
      "          6       0.79      0.85      0.99      0.82      0.92      0.83        71\n",
      "          7       1.00      0.33      1.00      0.50      0.58      0.31        15\n",
      "          8       0.57      0.48      0.97      0.52      0.68      0.44       184\n",
      "          9       0.80      0.57      1.00      0.67      0.75      0.54        86\n",
      "         10       0.21      0.14      0.98      0.17      0.37      0.13        72\n",
      "\n",
      "avg / total       0.78      0.78      0.91      0.78      0.83      0.70      2574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_0 = cross_val_predict(sgd_clf0, X_train_scaled, y_train_cat0, cv=3)\n",
    "print(classification_report_imbalanced(y_train_cat0, y_train_pred_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure confusion_matrix_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# since sklearn 0.22, you can use sklearn.metrics.plot_confusion_matrix()\n",
    "def plot_confusion_matrix(matrix):\n",
    "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "#plt.matshow(conf_mx, cmap=plt.cm.gray) # greyscale version\n",
    "plot_confusion_matrix(conf_mx)\n",
    "save_fig(\"confusion_matrix_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure confusion_matrix_errors_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "# plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plot_confusion_matrix(norm_conf_mx)\n",
    "save_fig(\"confusion_matrix_errors_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Classification for Random Forest\n",
    "the book doesn't deal with this, but states that the Random Forest algorithm is \"capable of handling multiple classes natively.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          A       0.80      0.77      0.99      0.79      0.87      0.75       159\n",
      "          B       0.53      0.34      1.00      0.42      0.59      0.32        29\n",
      "          C       0.92      0.95      0.91      0.94      0.93      0.87      1410\n",
      "          D       0.71      0.45      1.00      0.56      0.67      0.43        11\n",
      "          E       0.89      0.83      1.00      0.86      0.91      0.81        59\n",
      "          F       0.89      0.82      1.00      0.85      0.90      0.80        66\n",
      "          G       0.88      0.90      0.99      0.89      0.95      0.89       186\n",
      "          H       0.91      0.88      1.00      0.89      0.94      0.86        98\n",
      "          I       0.83      0.83      1.00      0.83      0.91      0.82        18\n",
      "          J       0.84      0.80      0.99      0.82      0.89      0.78       110\n",
      "          K       0.82      0.82      0.99      0.82      0.90      0.80        71\n",
      "          L       1.00      1.00      1.00      1.00      1.00      1.00        15\n",
      "          M       0.72      0.74      0.99      0.73      0.86      0.72       113\n",
      "          N       0.59      0.55      0.99      0.57      0.74      0.52        71\n",
      "          O       0.48      0.64      0.99      0.55      0.80      0.61        22\n",
      "          P       0.86      0.83      1.00      0.84      0.91      0.81        29\n",
      "          Q       0.81      0.74      1.00      0.78      0.86      0.72        35\n",
      "          R       0.76      0.59      1.00      0.67      0.77      0.57        27\n",
      "          S       0.49      0.42      0.99      0.45      0.65      0.40        40\n",
      "          T       0.60      0.75      1.00      0.67      0.87      0.73         4\n",
      "          U       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.87      0.87      0.94      0.87      0.90      0.81      2574\n",
      "\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          1       0.83      0.76      0.99      0.79      0.87      0.74       159\n",
      "          2       0.94      0.96      0.91      0.95      0.93      0.87      1509\n",
      "          3       0.93      0.76      1.00      0.83      0.87      0.74        66\n",
      "          4       0.88      0.90      0.98      0.89      0.94      0.88       302\n",
      "          5       0.81      0.81      0.99      0.81      0.90      0.79       110\n",
      "          6       0.81      0.83      0.99      0.82      0.91      0.81        71\n",
      "          7       1.00      1.00      1.00      1.00      1.00      1.00        15\n",
      "          8       0.72      0.70      0.98      0.71      0.83      0.67       184\n",
      "          9       0.78      0.79      0.99      0.79      0.89      0.77        86\n",
      "         10       0.64      0.53      0.99      0.58      0.72      0.50        72\n",
      "\n",
      "avg / total       0.89      0.89      0.94      0.89      0.91      0.83      2574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest_clf=RandomForestClassifier(n_estimators=100)\n",
    "y_train_pred_for1 = cross_val_predict(forest_clf, X_train_scaled, y_train_cat1, cv=3)\n",
    "y_train_pred_for0 = cross_val_predict(forest_clf, X_train_scaled, y_train_cat0, cv=3)\n",
    "y_train_pred_for2 = cross_val_predict(forest_clf, X_train_scaled, y_train_cat2, cv=3)\n",
    "\n",
    "print(classification_report_imbalanced(y_train_cat1, y_train_pred_for1))\n",
    "print(classification_report_imbalanced(y_train_cat0, y_train_pred_for0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          1       0.75      0.77      0.99      0.76      0.87      0.75       132\n",
      "          2       0.67      0.29      1.00      0.40      0.53      0.27         7\n",
      "          3       0.75      0.75      1.00      0.75      0.87      0.73        20\n",
      "          5       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "          6       0.00      0.00      1.00      0.00      0.00      0.00         3\n",
      "          7       1.00      0.71      1.00      0.83      0.85      0.69         7\n",
      "          8       0.63      0.80      1.00      0.71      0.89      0.78        15\n",
      "          9       0.67      0.67      1.00      0.67      0.82      0.64         3\n",
      "         10       0.77      0.73      0.99      0.75      0.85      0.70       156\n",
      "         11       0.69      0.69      1.00      0.69      0.83      0.67        13\n",
      "         12       1.00      0.75      1.00      0.86      0.87      0.73         4\n",
      "         13       0.68      0.75      0.99      0.71      0.86      0.72        68\n",
      "         14       0.82      0.86      1.00      0.84      0.93      0.85        37\n",
      "         15       0.64      0.58      1.00      0.61      0.76      0.56        24\n",
      "         16       0.53      0.51      0.99      0.52      0.71      0.48        39\n",
      "         17       0.74      0.66      1.00      0.70      0.81      0.63        47\n",
      "         18       0.40      0.22      1.00      0.29      0.47      0.20         9\n",
      "         19       0.27      0.29      0.99      0.28      0.53      0.26        21\n",
      "         20       0.72      0.76      0.98      0.74      0.86      0.73       140\n",
      "         21       0.57      0.47      1.00      0.52      0.69      0.44        17\n",
      "         22       0.40      0.45      0.99      0.43      0.67      0.42        38\n",
      "         23       0.51      0.51      0.99      0.51      0.71      0.48        70\n",
      "         24       0.85      0.86      0.99      0.86      0.93      0.85        95\n",
      "         25       0.61      0.60      0.99      0.60      0.77      0.57        72\n",
      "         26       0.72      0.77      0.99      0.75      0.87      0.74        99\n",
      "         27       0.74      0.70      0.99      0.72      0.83      0.68        84\n",
      "         28       0.69      0.67      0.98      0.68      0.81      0.64       187\n",
      "         29       0.48      0.47      0.99      0.47      0.68      0.44        30\n",
      "         30       0.62      0.64      0.99      0.63      0.80      0.61        44\n",
      "         31       0.58      0.58      1.00      0.58      0.76      0.55        19\n",
      "         32       0.48      0.47      0.99      0.48      0.68      0.44        55\n",
      "         33       0.74      0.62      1.00      0.68      0.79      0.59        42\n",
      "         35       0.71      0.45      1.00      0.56      0.67      0.43        11\n",
      "         36       0.33      0.67      1.00      0.44      0.82      0.64         3\n",
      "         37       0.67      1.00      1.00      0.80      1.00      1.00         2\n",
      "         38       0.89      0.80      1.00      0.84      0.89      0.78        49\n",
      "         39       0.62      1.00      1.00      0.77      1.00      1.00         5\n",
      "         41       0.88      0.83      1.00      0.86      0.91      0.82        18\n",
      "         42       0.62      0.57      1.00      0.59      0.75      0.54        23\n",
      "         43       0.63      0.68      1.00      0.65      0.82      0.66        25\n",
      "         45       0.67      0.79      0.99      0.72      0.89      0.77        33\n",
      "         46       0.68      0.76      0.99      0.72      0.87      0.73       100\n",
      "         47       0.51      0.43      0.99      0.47      0.66      0.41        53\n",
      "         49       0.64      0.81      0.99      0.71      0.90      0.79        31\n",
      "         50       0.89      0.85      1.00      0.87      0.92      0.84        20\n",
      "         51       0.83      0.91      1.00      0.87      0.95      0.90        11\n",
      "         52       0.65      0.54      1.00      0.59      0.73      0.51        28\n",
      "         53       1.00      0.75      1.00      0.86      0.87      0.73         8\n",
      "         55       0.88      0.78      1.00      0.82      0.88      0.76         9\n",
      "         56       0.70      0.78      1.00      0.74      0.88      0.76         9\n",
      "         58       0.67      0.73      0.99      0.70      0.85      0.71        41\n",
      "         59       0.54      0.56      1.00      0.55      0.75      0.53        25\n",
      "         60       0.67      0.57      1.00      0.62      0.76      0.55         7\n",
      "         61       0.94      1.00      1.00      0.97      1.00      1.00        17\n",
      "         62       0.33      0.30      1.00      0.32      0.55      0.28        10\n",
      "         63       0.67      0.40      1.00      0.50      0.63      0.38        10\n",
      "         64       0.71      0.74      1.00      0.72      0.86      0.72        23\n",
      "         65       0.84      0.87      1.00      0.85      0.93      0.85        30\n",
      "         66       0.28      0.39      0.99      0.33      0.62      0.36        18\n",
      "         68       1.00      1.00      1.00      1.00      1.00      1.00        15\n",
      "         69       0.29      0.18      1.00      0.22      0.43      0.17        11\n",
      "         70       0.56      0.56      1.00      0.56      0.74      0.53         9\n",
      "         71       0.71      0.65      1.00      0.68      0.81      0.63        23\n",
      "         72       0.93      1.00      1.00      0.96      1.00      1.00        38\n",
      "         73       0.89      0.67      1.00      0.76      0.82      0.64        12\n",
      "         74       0.45      0.47      1.00      0.46      0.69      0.45        19\n",
      "         75       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "         77       0.56      0.43      1.00      0.49      0.66      0.41        23\n",
      "         78       0.41      0.70      1.00      0.52      0.84      0.68        10\n",
      "         79       0.57      0.73      1.00      0.64      0.85      0.71        11\n",
      "         80       0.60      0.60      1.00      0.60      0.77      0.58         5\n",
      "         81       0.33      0.18      1.00      0.24      0.43      0.17        11\n",
      "         82       0.12      0.09      1.00      0.11      0.30      0.08        11\n",
      "         84       0.50      0.59      0.99      0.54      0.77      0.56        22\n",
      "         85       0.83      0.83      1.00      0.83      0.91      0.81        29\n",
      "         86       0.71      0.62      1.00      0.67      0.79      0.60        16\n",
      "         87       0.38      0.43      1.00      0.40      0.65      0.40         7\n",
      "         88       0.58      0.58      1.00      0.58      0.76      0.56        12\n",
      "         90       0.83      1.00      1.00      0.91      1.00      1.00         5\n",
      "         91       0.50      0.17      1.00      0.25      0.41      0.15         6\n",
      "         92       1.00      0.83      1.00      0.91      0.91      0.82         6\n",
      "         93       0.33      0.30      1.00      0.32      0.55      0.28        10\n",
      "         94       0.77      0.71      1.00      0.74      0.84      0.69        14\n",
      "         95       0.18      0.20      1.00      0.19      0.45      0.18        10\n",
      "         96       0.31      0.31      1.00      0.31      0.56      0.29        16\n",
      "         97       1.00      1.00      1.00      1.00      1.00      1.00         2\n",
      "         98       0.33      0.50      1.00      0.40      0.71      0.47         2\n",
      "         99       0.00      0.00      1.00      0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.67      0.67      0.99      0.67      0.81      0.65      2574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report_imbalanced(y_train_cat2, y_train_pred_for2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel classification\n",
    "This is where there is more than one class. So in our case, we could check for Category_0 and Category_1. However, each category_1 class always belongs to the same Category_0 class, so this is a special case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train_C = (y_train_cat1 == 'C')\n",
    "y_train_2 = (y_train_cat0 == 2)\n",
    "\n",
    "y_multilabel = np.c_[y_train_C, y_train_2]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.predict([test_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.931940675708821"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This code computes the average F1 score across all labels: it can take a long time but not here.\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9321046659125645"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This time it's a weighted average across all labels\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I presume it's ok to use the imbalanced learn metrics here too..   \n",
    "... acutally, no:   \n",
    "ValueError: imblearn does not support multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it do on prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [False, False],\n",
       "       [False, False],\n",
       "       ...,\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_knn_pred = knn_clf.predict(X_train)\n",
    "y_knn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False],\n",
       "       [False, False],\n",
       "       [False, False],\n",
       "       ...,\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_result = y_multilabel.copy()\n",
    "#knn_result['result'] = y_knn_pred\n",
    "knn_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       ...,\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_knn_pred == y_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_C = (y_train_cat1 == 'C')\n",
    "y_train_2 = (y_train_cat0 == 2)\n",
    "y_train_10 = (y_train_cat2 == 10)\n",
    "\n",
    "y_multilabel = np.c_[y_train_C, y_train_2, y_train_10]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9211046676852388"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_multilabel, y_train_knn_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.predict([test_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.24176407,  1.495134  ,  6.934062  , ...,  7.17451763,\n",
       "         6.48560524,  4.21679831],\n",
       "       [ 6.43087292,  2.48803687,  1.78880703, ...,  3.84727097,\n",
       "         6.04655981,  1.59090734],\n",
       "       [10.60281658,  1.62183428,  7.01505661, ...,  8.39739513,\n",
       "         6.74059629,  2.91967773],\n",
       "       ...,\n",
       "       [10.30541515,  1.78580189,  6.96380043, ...,  5.42338753,\n",
       "         4.96495056,  7.99304438],\n",
       "       [ 9.45425415,  2.33843803,  7.2139535 , ...,  7.08365107,\n",
       "         6.09522724,  6.15777254],\n",
       "       [ 6.34459972,  1.76774478,  1.20839155, ...,  3.46893382,\n",
       "         6.57862043,  2.12571478]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
